{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60b09a19-3dfa-43e9-8f11-5fb3ab759392",
   "metadata": {},
   "source": [
    "# <span style=\"color:black;\">**Word Prediction using RNN and LSTM**</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d327718f-6619-4fd4-a47d-28694dfc40e5",
   "metadata": {},
   "source": [
    "## <span style=\"color:black\">**Overview**</span>\n",
    "\n",
    "Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM) networks are powerful tools for sequential data tasks, such as natural language processing. This project demonstrates how to use these models to predict the next word in a sentence, leveraging the ability of RNNs and LSTMs to maintain context over sequences.\n",
    "\n",
    "# <span style=\"color:black;\">**Features**</span>\n",
    "\n",
    "- **Implementation of RNN for sentence word prediction**: Build and train a basic RNN model to predict the next word in a sentence.\n",
    "- **Implementation of LSTM for sentence word prediction**: Enhance the RNN model with LSTM layers to better capture long-term dependencies in the text.\n",
    "- **Training and evaluation scripts**: Scripts to train the models and evaluate their performance on the test data.\n",
    "- **Preprocessing and tokenization of text data**: Techniques for preparing the text data, including tokenization and padding to ensure consistent input lengths.\n",
    "- **Model saving and loading functionality**: Save the trained models to disk and load them for future predictions or further training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bea341e7-5ac7-495a-9f22-b9da725a336f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow. keras.models import Sequential\n",
    "from tensorflow. keras. layers import Dense, SimpleRNN, LSTM, Embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5ef2bd-f484-481b-a8b4-570cc816ab07",
   "metadata": {},
   "source": [
    "## **Dataset Overview**\n",
    "\n",
    "### **Description**\n",
    "\n",
    "This dataset consists of individual sentences related to various concepts and applications in artificial intelligence (AI), machine learning (ML), and related fields. Each sentence provides a definition, explanation, or concept related to these domains.\n",
    "\n",
    "### **Features of the Dataset**\n",
    "\n",
    "1. **Comprehensive Coverage**: The sentences cover a wide range of topics, including different types of algorithms, learning paradigms, specific AI techniques, and their applications.\n",
    "2. **Educational Use**: It serves as a useful resource for learning and understanding various AI and ML concepts.\n",
    "3. **Versatile Applications**: The dataset can be applied in various NLP tasks such as text generation, classification, and summarization.\n",
    "\n",
    "### **Possible Uses**\n",
    "\n",
    "- **Text Generation**: Train a model to generate sentences or paragraphs on AI topics.\n",
    "- **Conceptual Understanding**: Use the dataset to build tools or applications that explain AI concepts to users.\n",
    "- **Content Classification**: Develop classifiers to categorize sentences into different AI-related topics or domains.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d1a86012-99da-4ea3-a132-33072b6d9f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"Machine learning algorithms improve through experience.\"\n",
    "    \"Neural networks are inspired by biological neural networks.\"\n",
    "    \"Deep learning is a subset of machine learning.\"\n",
    "    \"Artificial intelligence aims to create intelligent machines.\"\n",
    "    \"Supervised learning uses labeled training data.\"\n",
    "    \"Unsupervised learning finds patterns in unlabeled data.\"\n",
    "    \"Reinforcement learning learns through interaction with an environment.\"\n",
    "    \"Natural language processing enables machines to understand human language.\"\n",
    "    \"Computer vision allows machines to interpret visual information.\"\n",
    "    \"Convolutional neural networks excel at image recognition tasks.\"\n",
    "    \"Recurrent neural networks are used for sequential data processing.\"\n",
    "    \"Support vector machines are effective for classification problems.\"\n",
    "    \"Decision trees are used for both classification and regression tasks.\"\n",
    "    \"Random forests combine multiple decision trees for improved accuracy.\"\n",
    "    \"Gradient boosting is an ensemble learning technique.\"\n",
    "    \"K-means clustering is an unsupervised learning algorithm.\"\n",
    "    \"Principal component analysis is used for dimensionality reduction.\"\n",
    "    \"Genetic algorithms are inspired by natural selection.\"\n",
    "    \"Artificial neural networks are composed of interconnected nodes.\"\n",
    "    \"Backpropagation is used to train neural networks.\"\n",
    "    \"Transfer learning leverages knowledge from pre-trained models.\"\n",
    "    \"Generative adversarial networks create new data samples.\"\n",
    "    \"Long short-term memory networks are used for time series analysis.\"\n",
    "    \"Autoencoders are used for feature learning and dimensionality reduction.\"\n",
    "    \"Ensemble methods combine multiple models for better predictions.\"\n",
    "    \"Overfitting occurs when a model performs well on training data but poorly on new data.\"\n",
    "    \"Cross-validation helps assess a model's performance on unseen data.\"\n",
    "    \"Hyperparameter tuning optimizes model performance.\"\n",
    "    \"Feature engineering creates new features from existing data.\"\n",
    "    \"Data preprocessing is crucial for successful machine learning.\"\n",
    "    \"Bias-variance tradeoff is a fundamental concept in machine learning.\"\n",
    "    \"Confusion matrices evaluate classification model performance.\"\n",
    "    \"ROC curves visualize classifier performance across different thresholds.\"\n",
    "    \"t-SNE is used for visualizing high-dimensional data.\"\n",
    "    \"Word embeddings represent words as vectors in a continuous space.\"\n",
    "    \"Sentiment analysis determines the emotional tone of text.\"\n",
    "    \"Recommender systems suggest items based on user preferences.\"\n",
    "    \"Anomaly detection identifies unusual patterns in data.\"\n",
    "    \"Reinforcement learning agents learn through trial and error.\"\n",
    "    \"Q-learning is a model-free reinforcement learning algorithm.\"\n",
    "    \"Markov decision processes model decision-making in uncertain environments.\"\n",
    "    \"Bayesian networks represent probabilistic relationships among variables.\"\n",
    "    \"Fuzzy logic allows for reasoning based on 'degrees of truth'.\"\n",
    "    \"Expert systems emulate human expert decision-making.\"\n",
    "    \"Knowledge representation is fundamental to artificial intelligence.\"\n",
    "    \"Heuristic search algorithms find approximate solutions to complex problems.\"\n",
    "    \"A* search algorithm is used for pathfinding and graph traversal.\"\n",
    "    \"Minimax algorithm is used in game theory and decision making.\"\n",
    "    \"Alpha-beta pruning optimizes the minimax algorithm.\"\n",
    "    \"Monte Carlo tree search is used in game AI.\"\n",
    "    \"Evolutionary algorithms solve optimization problems inspired by natural evolution.\"\n",
    "    \"Swarm intelligence algorithms are inspired by collective behavior in nature.\"\n",
    "    \"Self-organizing maps are used for dimensionality reduction and visualization.\"\n",
    "    \"Boltzmann machines are stochastic recurrent neural networks.\"\n",
    "    \"Restricted Boltzmann machines are used for dimensionality reduction and feature learning.\"\n",
    "    \"Deep belief networks are composed of multiple layers of latent variables.\"\n",
    "    \"Capsule networks aim to improve upon traditional convolutional neural networks.\"\n",
    "    \"Attention mechanisms allow models to focus on specific parts of input data.\"\n",
    "    \"Transformer models have revolutionized natural language processing tasks.\"\n",
    "    \"BERT is a transformer-based model for natural language understanding.\"\n",
    "    \"GPT (Generative Pre-trained Transformer) models generate human-like text.\"\n",
    "    \"Few-shot learning aims to learn from a small number of examples.\"\n",
    "    \"Zero-shot learning classifies instances of classes not seen during training.\"\n",
    "    \"Meta-learning involves learning how to learn efficiently.\"\n",
    "    \"Federated learning allows training models on distributed data sources.\"\n",
    "    \"Edge AI brings artificial intelligence capabilities to edge devices.\"\n",
    "    \"Explainable AI aims to make AI systems' decisions interpretable.\"\n",
    "    \"Adversarial machine learning studies vulnerabilities of AI systems.\"\n",
    "    \"Quantum machine learning explores quantum computing for AI tasks.\"\n",
    "    \"Neuromorphic computing aims to mimic biological neural systems.\"\n",
    "    \"Automated machine learning (AutoML) automates the process of applying machine learning.\"\n",
    "    \"Ethical AI focuses on developing AI systems that are fair and unbiased.\"\n",
    "    \"Computer-generated art uses AI to create original artworks.\"\n",
    "    \"AI-powered robotics combines AI with physical machines.\"\n",
    "    \"Conversational AI enables natural language interactions with machines.\"\n",
    "    \"Speech recognition converts spoken language into text.\"\n",
    "    \"Text-to-speech systems convert written text into spoken words.\"\n",
    "    \"Object detection identifies and locates objects in images or videos.\"\n",
    "    \"Semantic segmentation classifies each pixel in an image.\"\n",
    "    \"Instance segmentation identifies and delineates each object instance.\"\n",
    "    \"Facial recognition identifies or verifies a person from their face.\"\n",
    "    \"Emotion recognition detects human emotions from facial expressions or voice.\"\n",
    "    \"Gesture recognition interprets human gestures via mathematical algorithms.\"\n",
    "    \"Autonomous vehicles use AI for navigation and decision-making.\"\n",
    "    \"Predictive maintenance uses AI to predict equipment failures.\"\n",
    "    \"Fraud detection employs AI to identify fraudulent activities.\"\n",
    "    \"AI in healthcare assists in diagnosis and treatment planning.\"\n",
    "    \"Bioinformatics uses AI for analyzing biological data.\"\n",
    "    \"AI in finance is used for algorithmic trading and risk assessment.\"\n",
    "    \"Computational creativity explores AI's potential for creative tasks.\"\n",
    "    \"AI ethics addresses moral and societal implications of AI.\"\n",
    "    \"Artificial general intelligence aims to match human-level intelligence.\"\n",
    "    \"Narrow AI specializes in specific tasks.\"\n",
    "    \"The Turing test assesses a machine's ability to exhibit intelligent behavior.\"\n",
    "    \"Machine perception deals with how machines understand sensory input.\"\n",
    "    \"Cognitive computing aims to simulate human thought processes.\"\n",
    "    \"AI alignment ensures AI systems' goals are aligned with human values.\"\n",
    "    \"Robotic process automation uses AI to automate repetitive tasks.\"\n",
    "    \"AI augmentation enhances human intelligence rather than replacing it.\"\n",
    "    \"The singularity refers to the hypothetical future creation of superintelligent AI.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acd91ba-eb94-4be4-aa58-cc2d7ec79b2a",
   "metadata": {},
   "source": [
    "### **Word Tokenization and Sequencing**\n",
    "\n",
    "The `Tokenizer` is used to convert words in the provided sentences into unique indices, allowing us to map each word to a numerical representation. The `total_words` variable indicates the total number of unique words plus one for padding. The `input_sequences` list is then prepared to store sequences of these word indices for further use in model training or analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0bf90bf6-c8e7-48e2-8dc1-0e31e352b919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "435\n",
      "{'ai': 1, 'learning': 2, 'to': 3, 'for': 4, 'are': 5, 'is': 6, 'data': 7, 'in': 8, 'and': 9, 'networks': 10, 'of': 11, 'used': 12, 'a': 13, 'machine': 14, 'neural': 15, 'machines': 16, 'human': 17, 'on': 18, 'intelligence': 19, 'tasks': 20, 'decision': 21, 'algorithms': 22, 'aims': 23, 'natural': 24, 'language': 25, 'models': 26, 'model': 27, 'the': 28, 'systems': 29, 'artificial': 30, 'uses': 31, 'with': 32, 'recognition': 33, 'algorithm': 34, 'from': 35, 'text': 36, 'inspired': 37, 'by': 38, 'training': 39, 'an': 40, 'dimensionality': 41, 'reduction': 42, 'performance': 43, 'identifies': 44, 'making': 45, 'through': 46, 'biological': 47, 'create': 48, 'reinforcement': 49, 'processing': 50, 'allows': 51, 'classification': 52, 'problems': 53, 'multiple': 54, 'analysis': 55, 'new': 56, 'feature': 57, 'based': 58, 'detection': 59, 'learn': 60, 'search': 61, 'transformer': 62, 'computing': 63, 'or': 64, 'improve': 65, 'deep': 66, 'intelligent': 67, 'unsupervised': 68, 'patterns': 69, 'enables': 70, 'understand': 71, 'computer': 72, 'convolutional': 73, 'image': 74, 'recurrent': 75, 'trees': 76, 'combine': 77, 'ensemble': 78, 'composed': 79, 'knowledge': 80, 'pre': 81, 'trained': 82, 'generative': 83, 'adversarial': 84, 'optimizes': 85, 'fundamental': 86, 'represent': 87, 'words': 88, 'processes': 89, 'variables': 90, 'expert': 91, 'minimax': 92, 'game': 93, 'behavior': 94, 'boltzmann': 95, 'specific': 96, 'input': 97, 'shot': 98, 'classifies': 99, 'how': 100, 'edge': 101, \"systems'\": 102, 'quantum': 103, 'explores': 104, 'process': 105, 'speech': 106, 'spoken': 107, 'into': 108, 'object': 109, 'segmentation': 110, 'each': 111, 'instance': 112, 'facial': 113, 'experience': 114, 'subset': 115, 'supervised': 116, 'labeled': 117, 'finds': 118, 'unlabeled': 119, 'learns': 120, 'interaction': 121, 'environment': 122, 'vision': 123, 'interpret': 124, 'visual': 125, 'information': 126, 'excel': 127, 'at': 128, 'sequential': 129, 'support': 130, 'vector': 131, 'effective': 132, 'both': 133, 'regression': 134, 'random': 135, 'forests': 136, 'improved': 137, 'accuracy': 138, 'gradient': 139, 'boosting': 140, 'technique': 141, 'k': 142, 'means': 143, 'clustering': 144, 'principal': 145, 'component': 146, 'genetic': 147, 'selection': 148, 'interconnected': 149, 'nodes': 150, 'backpropagation': 151, 'train': 152, 'transfer': 153, 'leverages': 154, 'samples': 155, 'long': 156, 'short': 157, 'term': 158, 'memory': 159, 'time': 160, 'series': 161, 'autoencoders': 162, 'methods': 163, 'better': 164, 'predictions': 165, 'overfitting': 166, 'occurs': 167, 'when': 168, 'performs': 169, 'well': 170, 'but': 171, 'poorly': 172, 'cross': 173, 'validation': 174, 'helps': 175, 'assess': 176, \"model's\": 177, 'unseen': 178, 'hyperparameter': 179, 'tuning': 180, 'engineering': 181, 'creates': 182, 'features': 183, 'existing': 184, 'preprocessing': 185, 'crucial': 186, 'successful': 187, 'bias': 188, 'variance': 189, 'tradeoff': 190, 'concept': 191, 'confusion': 192, 'matrices': 193, 'evaluate': 194, 'roc': 195, 'curves': 196, 'visualize': 197, 'classifier': 198, 'across': 199, 'different': 200, 'thresholds': 201, 't': 202, 'sne': 203, 'visualizing': 204, 'high': 205, 'dimensional': 206, 'word': 207, 'embeddings': 208, 'as': 209, 'vectors': 210, 'continuous': 211, 'space': 212, 'sentiment': 213, 'determines': 214, 'emotional': 215, 'tone': 216, 'recommender': 217, 'suggest': 218, 'items': 219, 'user': 220, 'preferences': 221, 'anomaly': 222, 'unusual': 223, 'agents': 224, 'trial': 225, 'error': 226, 'q': 227, 'free': 228, 'markov': 229, 'uncertain': 230, 'environments': 231, 'bayesian': 232, 'probabilistic': 233, 'relationships': 234, 'among': 235, 'fuzzy': 236, 'logic': 237, 'reasoning': 238, \"'degrees\": 239, \"truth'\": 240, 'emulate': 241, 'representation': 242, 'heuristic': 243, 'find': 244, 'approximate': 245, 'solutions': 246, 'complex': 247, 'pathfinding': 248, 'graph': 249, 'traversal': 250, 'theory': 251, 'alpha': 252, 'beta': 253, 'pruning': 254, 'monte': 255, 'carlo': 256, 'tree': 257, 'evolutionary': 258, 'solve': 259, 'optimization': 260, 'evolution': 261, 'swarm': 262, 'collective': 263, 'nature': 264, 'self': 265, 'organizing': 266, 'maps': 267, 'visualization': 268, 'stochastic': 269, 'restricted': 270, 'belief': 271, 'layers': 272, 'latent': 273, 'capsule': 274, 'aim': 275, 'upon': 276, 'traditional': 277, 'attention': 278, 'mechanisms': 279, 'allow': 280, 'focus': 281, 'parts': 282, 'have': 283, 'revolutionized': 284, 'bert': 285, 'understanding': 286, 'gpt': 287, 'generate': 288, 'like': 289, 'few': 290, 'small': 291, 'number': 292, 'examples': 293, 'zero': 294, 'instances': 295, 'classes': 296, 'not': 297, 'seen': 298, 'during': 299, 'meta': 300, 'involves': 301, 'efficiently': 302, 'federated': 303, 'distributed': 304, 'sources': 305, 'brings': 306, 'capabilities': 307, 'devices': 308, 'explainable': 309, 'make': 310, 'decisions': 311, 'interpretable': 312, 'studies': 313, 'vulnerabilities': 314, 'neuromorphic': 315, 'mimic': 316, 'automated': 317, 'automl': 318, 'automates': 319, 'applying': 320, 'ethical': 321, 'focuses': 322, 'developing': 323, 'that': 324, 'fair': 325, 'unbiased': 326, 'generated': 327, 'art': 328, 'original': 329, 'artworks': 330, 'powered': 331, 'robotics': 332, 'combines': 333, 'physical': 334, 'conversational': 335, 'interactions': 336, 'converts': 337, 'convert': 338, 'written': 339, 'locates': 340, 'objects': 341, 'images': 342, 'videos': 343, 'semantic': 344, 'pixel': 345, 'delineates': 346, 'verifies': 347, 'person': 348, 'their': 349, 'face': 350, 'emotion': 351, 'detects': 352, 'emotions': 353, 'expressions': 354, 'voice': 355, 'gesture': 356, 'interprets': 357, 'gestures': 358, 'via': 359, 'mathematical': 360, 'autonomous': 361, 'vehicles': 362, 'use': 363, 'navigation': 364, 'predictive': 365, 'maintenance': 366, 'predict': 367, 'equipment': 368, 'failures': 369, 'fraud': 370, 'employs': 371, 'identify': 372, 'fraudulent': 373, 'activities': 374, 'healthcare': 375, 'assists': 376, 'diagnosis': 377, 'treatment': 378, 'planning': 379, 'bioinformatics': 380, 'analyzing': 381, 'finance': 382, 'algorithmic': 383, 'trading': 384, 'risk': 385, 'assessment': 386, 'computational': 387, 'creativity': 388, \"ai's\": 389, 'potential': 390, 'creative': 391, 'ethics': 392, 'addresses': 393, 'moral': 394, 'societal': 395, 'implications': 396, 'general': 397, 'match': 398, 'level': 399, 'narrow': 400, 'specializes': 401, 'turing': 402, 'test': 403, 'assesses': 404, \"machine's\": 405, 'ability': 406, 'exhibit': 407, 'perception': 408, 'deals': 409, 'sensory': 410, 'cognitive': 411, 'simulate': 412, 'thought': 413, 'alignment': 414, 'ensures': 415, 'goals': 416, 'aligned': 417, 'values': 418, 'robotic': 419, 'automation': 420, 'automate': 421, 'repetitive': 422, 'augmentation': 423, 'enhances': 424, 'rather': 425, 'than': 426, 'replacing': 427, 'it': 428, 'singularity': 429, 'refers': 430, 'hypothetical': 431, 'future': 432, 'creation': 433, 'superintelligent': 434}\n"
     ]
    }
   ],
   "source": [
    "#Assign number or index to each word\n",
    "tokenizer = Tokenizer()\n",
    "# Fit the tokenizer on the provided sentences\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "# Get the total number of unique words (plus one for padding)\n",
    "total_words = len (tokenizer.word_index) + 1\n",
    "print (total_words)\n",
    "#unique words\n",
    "print (tokenizer.word_index)\n",
    "# Initialize a list to hold input sequences\n",
    "input_sequences = []\n",
    "# Iterate over each sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88dca2d-d8a9-48d0-a7e2-722cb905de2f",
   "metadata": {},
   "source": [
    "### **Generating N-Gram Sequences**\n",
    "\n",
    "The code iterates over each sentence to convert it into a sequence of integers using the tokenizer. For each sequence, it generates n-gram sequences by creating all possible combinations of words within the sentence. These n-gram sequences are appended to the `input_sequences` list for use in model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cf7799ed-d624-45fc-bbdc-28536767250d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over each sentence\n",
    "for line in sentences:\n",
    "  # Convert the sentence to a sequence of integers\n",
    "  token_list = tokenizer.texts_to_sequences( [line]) [0]\n",
    "  # print (token_list)\n",
    "  # Create n-gram sequences (generate possiable combination of words in sentance)\n",
    "  for i in range(1, len(token_list)):\n",
    "    n_gram_sequence = token_list[: i+1]\n",
    "    input_sequences. append (n_gram_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285da47a-b6e8-4a4e-9fdc-6a3920e58b6a",
   "metadata": {},
   "source": [
    "## **Sequence Padding and Length Determination**\n",
    "\n",
    "The `max_sequence_len` variable is calculated to determine the length of the longest sequence in the dataset. The `pad_sequences` function then pads all sequences to this maximum length, ensuring uniform input size for the model. The padded sequences are converted into a NumPy array and printed, showing each sequence aligned to the same length with padding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5568af13-5e4a-40e5-9e35-a06ccf487bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0 ...   0  14   2]\n",
      " [  0   0   0 ...  14   2  22]\n",
      " [  0   0   0 ...   2  22  65]\n",
      " ...\n",
      " [  0   0  14 ... 432 433  11]\n",
      " [  0  14   2 ... 433  11 434]\n",
      " [ 14   2  22 ...  11 434   1]]\n"
     ]
    }
   ],
   "source": [
    "# Determine the maximum sequence length\n",
    "max_sequence_len = max([len(x) for x in input_sequences] )\n",
    "# Pad sequences to ensure they are all the same length\n",
    "input_sequences = np.array (pad_sequences (input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "#n-gram seq\n",
    "print (input_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc57dbca-9e09-463b-ae5c-dd5191ebcb48",
   "metadata": {},
   "source": [
    "## **Preparing Input and Output Data**\n",
    "\n",
    "The code separates the sequences into input (`X`) and output (`y`) components. `X` contains all elements of the sequences except the last one, used as input data for the model. `y` consists of the last element of each sequence, which represents the target output. Both `X` and `y` are then prepared for model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1497ebda-ff56-4799-8189-745b6c0f4f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Data [[  0   0   0 ...   0   0  14]\n",
      " [  0   0   0 ...   0  14   2]\n",
      " [  0   0   0 ...  14   2  22]\n",
      " ...\n",
      " [  0   0  14 ... 431 432 433]\n",
      " [  0  14   2 ... 432 433  11]\n",
      " [ 14   2  22 ... 433  11 434]]\n"
     ]
    }
   ],
   "source": [
    "# all elements of seq except the last one\n",
    "X = input_sequences [:, :-1]\n",
    "print (\"Input Data\",X)\n",
    "#the last element of each seq\n",
    "y = input_sequences [:, -1]\n",
    "#print (\"Output Data\",y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96214754-1d7c-417c-847e-ddf7584624da",
   "metadata": {},
   "source": [
    "### **One-Hot Encoding**\n",
    "\n",
    "The `to_categorical` function from TensorFlow is used to convert the output labels (`y`) into one-hot encoded vectors. This transforms the numerical labels into a binary matrix representation, where each label is represented as a vector with a single high value (1) corresponding to the class, and all other values are zero. This encoding is done for `total_words` classes, which corresponds to the number of unique words in the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d58c83bc-55c4-4a01-a840-8981a5553b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#one hot encoding,predict word out of num clasess= unqiue words 435\n",
    "y = tf.keras.utils.to_categorical(y, num_classes=total_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935a155b-849b-4e71-88b6-78764e777d5b",
   "metadata": {},
   "source": [
    "### **Define the RNN Model**\n",
    "\n",
    "The model is a Sequential network where layers are stacked linearly. \n",
    "\n",
    "1. **Embedding Layer**: Converts word indices into dense vectors with a dimensionality of 10. The input dimension is the total number of unique words, and the input length is `max_sequence_len - 1`. This layer transforms each word index into a 10-dimensional vector.\n",
    "\n",
    "2. **RNN Layer**: A `SimpleRNN` layer with 30 units processes the sequences and captures temporal dependencies.\n",
    "\n",
    "3. **Dense Output Layer**: A `Dense` layer with `total_words` units and a softmax activation function is used for multi-class classification. This layer outputs probabilities for each word in the vocabulary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d43aeee2-6344-41fc-a7b1-fbe5236b1320",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ubaid-khan\\anaconda3\\envs\\MytestEnv\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Define the RNN model\n",
    "#Sequential model allows stacking layers in a linear fashion\n",
    "model = Sequential([\n",
    "  # Embedding layer to convert word indices to dense vectors o\n",
    "  # Input dimension: total number of words, Output dimension:\n",
    "  # Input length: length of input sequences (excluding the las\n",
    "  # 10 means, 10 diamention vector, feature save in the 10 diamention vector, if you have 1000 sentences the resultant vector size 1000*10\n",
    "  #max_sequence_len, if I have sentance of 5 words than I will give 4 words as input, 5th will be pridict by model\n",
    "  Embedding (total_words, 10, input_length=max_sequence_len-1),\n",
    "  #RNN 30 seq of cells\n",
    "  SimpleRNN(30),\n",
    "  # Dense output layer with a softmax activation function\n",
    "  # Output dimension: total number of words (for multi-class\n",
    "  # softmax is the last layer\n",
    "  Dense(total_words, activation='softmax')\n",
    "  ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6d8e10d3-82d9-430c-b373-40f8d4a9cf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile( loss='categorical_crossentropy',optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "95010511-c7b0-408a-b6f9-1ea241b61322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 112ms/step - accuracy: 0.0037 - loss: 6.0738\n",
      "Epoch 2/50\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 111ms/step - accuracy: 0.0124 - loss: 6.0439\n",
      "Epoch 3/50\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 110ms/step - accuracy: 0.0384 - loss: 6.0040\n",
      "Epoch 4/50\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 114ms/step - accuracy: 0.0430 - loss: 5.9080\n",
      "Epoch 5/50\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 116ms/step - accuracy: 0.0332 - loss: 5.7642\n",
      "Epoch 6/50\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 115ms/step - accuracy: 0.0250 - loss: 5.6476\n",
      "Epoch 7/50\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 129ms/step - accuracy: 0.0208 - loss: 5.5763\n",
      "Epoch 8/50\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 116ms/step - accuracy: 0.0306 - loss: 5.5946\n",
      "Epoch 9/50\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 123ms/step - accuracy: 0.0323 - loss: 5.5707\n",
      "Epoch 10/50\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 155ms/step - accuracy: 0.0279 - loss: 5.5548\n",
      "Epoch 11/50\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 122ms/step - accuracy: 0.0282 - loss: 5.5413\n",
      "Epoch 12/50\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 140ms/step - accuracy: 0.0379 - loss: 5.5389\n",
      "Epoch 13/50\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 158ms/step - accuracy: 0.0335 - loss: 5.5402\n",
      "Epoch 14/50\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 154ms/step - accuracy: 0.0358 - loss: 5.4620\n",
      "Epoch 15/50\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 138ms/step - accuracy: 0.0515 - loss: 5.4773\n",
      "Epoch 16/50\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 122ms/step - accuracy: 0.0465 - loss: 5.5139\n",
      "Epoch 17/50\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 161ms/step - accuracy: 0.0481 - loss: 5.4523\n",
      "Epoch 18/50\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 123ms/step - accuracy: 0.0503 - loss: 5.3612\n",
      "Epoch 19/50\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 129ms/step - accuracy: 0.0598 - loss: 5.3167\n",
      "Epoch 20/50\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 139ms/step - accuracy: 0.0635 - loss: 5.2502\n",
      "Epoch 21/50\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 137ms/step - accuracy: 0.0731 - loss: 5.1983\n",
      "Epoch 22/50\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 130ms/step - accuracy: 0.0626 - loss: 5.1039\n",
      "Epoch 23/50\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 154ms/step - accuracy: 0.0764 - loss: 5.0865\n",
      "Epoch 24/50\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 143ms/step - accuracy: 0.0940 - loss: 4.9430\n",
      "Epoch 25/50\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 128ms/step - accuracy: 0.1073 - loss: 4.9017\n",
      "Epoch 26/50\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 146ms/step - accuracy: 0.1008 - loss: 4.8374\n",
      "Epoch 27/50\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 115ms/step - accuracy: 0.1175 - loss: 4.7463\n",
      "Epoch 28/50\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 121ms/step - accuracy: 0.0954 - loss: 4.7495\n",
      "Epoch 29/50\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 162ms/step - accuracy: 0.1213 - loss: 4.5977\n",
      "Epoch 30/50\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 123ms/step - accuracy: 0.1564 - loss: 4.5273\n",
      "Epoch 31/50\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 139ms/step - accuracy: 0.1633 - loss: 4.4698\n",
      "Epoch 32/50\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 151ms/step - accuracy: 0.1716 - loss: 4.4223\n",
      "Epoch 33/50\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 127ms/step - accuracy: 0.1775 - loss: 4.3387\n",
      "Epoch 34/50\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 123ms/step - accuracy: 0.2042 - loss: 4.2137\n",
      "Epoch 35/50\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 156ms/step - accuracy: 0.2206 - loss: 4.1976\n",
      "Epoch 36/50\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 125ms/step - accuracy: 0.2127 - loss: 4.0937\n",
      "Epoch 37/50\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 122ms/step - accuracy: 0.2424 - loss: 4.0496\n",
      "Epoch 38/50\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 138ms/step - accuracy: 0.2708 - loss: 3.9120\n",
      "Epoch 39/50\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 134ms/step - accuracy: 0.2718 - loss: 3.8821\n",
      "Epoch 40/50\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 114ms/step - accuracy: 0.2748 - loss: 3.8148\n",
      "Epoch 41/50\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 150ms/step - accuracy: 0.3129 - loss: 3.6839\n",
      "Epoch 42/50\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 127ms/step - accuracy: 0.3133 - loss: 3.6118\n",
      "Epoch 43/50\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 147ms/step - accuracy: 0.3277 - loss: 3.5703\n",
      "Epoch 44/50\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 138ms/step - accuracy: 0.3716 - loss: 3.4959\n",
      "Epoch 45/50\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 131ms/step - accuracy: 0.3795 - loss: 3.4531\n",
      "Epoch 46/50\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 174ms/step - accuracy: 0.3687 - loss: 3.3503\n",
      "Epoch 47/50\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 142ms/step - accuracy: 0.4332 - loss: 3.2541\n",
      "Epoch 48/50\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 124ms/step - accuracy: 0.3831 - loss: 3.2470\n",
      "Epoch 49/50\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 159ms/step - accuracy: 0.4044 - loss: 3.1656\n",
      "Epoch 50/50\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 141ms/step - accuracy: 0.4197 - loss: 3.1392\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2b32961fd70>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=50, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0832e41f-1bfd-4ca4-b8fb-5834f3a449c3",
   "metadata": {},
   "source": [
    "### **Function to Predict the Next Word(s)**\n",
    "\n",
    "The `predict_next_word` function generates text based on a seed text input.\n",
    "\n",
    "1. **Convert to Sequence**: The seed text is converted into a sequence of integers using the tokenizer and padded to the required input length.\n",
    "\n",
    "2. **Predict Next Word**: The model predicts the probabilities for the next word, and the word with the highest probability is selected.\n",
    "\n",
    "3. **Update Seed Text**: The predicted word is appended to the seed text, and the process can be repeated for multiple words as specified.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0f8a557d-b92e-4bf2-a5e8-6c53132591a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict the next word(s) given a seed text\n",
    "def predict_next_word(seed_text, next_words=1):\n",
    "  for _ in range(next_words):\n",
    "    # Convert the seed text to a sequence of integers\n",
    "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    # Pad the sequence to match the input length required\n",
    "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "    # Predict the probabilities of the next word in the se\n",
    "    predicted = model.predict(token_list, verbose=0)\n",
    "    # Get the index of the word with the highest probability\n",
    "    predicted_word_index = np.argmax(predicted, axis=-1)[0]\n",
    "    predicted_word = tokenizer.index_word[predicted_word_index]\n",
    "    # Append the predicted word to the seed text\n",
    "    seed_text += \" \" + predicted_word\n",
    "    # Get the word corresponding to the predicted index\n",
    "  return seed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427c8fd1-a81e-4724-9bce-0725a15464d2",
   "metadata": {},
   "source": [
    "### **Predict and Print Next Word**\n",
    "\n",
    "To predict and print the next word(s) given a seed text, call the `predict_next_word` function with the desired seed text and number of words to predict. For example:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "024fabe5-c6e5-4a9b-9374-85864908c612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "machine learning ai to language\n"
     ]
    }
   ],
   "source": [
    "print(predict_next_word(\"machine learning ai to\"))\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744e6b10-5410-43ee-b0b0-c6e64c798906",
   "metadata": {},
   "source": [
    "### **Define the LSTM Model**\n",
    "\n",
    "The model is a Sequential network with layers stacked linearly:\n",
    "\n",
    "1. **Embedding Layer**: Converts word indices into dense vectors with a dimensionality of 10. The input dimension is the total number of unique words, and the input length is `max_sequence_len - 1`. This layer transforms each word index into a 10-dimensional vector.\n",
    "\n",
    "2. **LSTM Layer**: An `LSTM` layer with 100 units processes the sequences, capturing long-term dependencies and patterns.\n",
    "\n",
    "3. **Dense Output Layer**: A `Dense` layer with `total_words` units and a softmax activation function is used for multi-class classification, outputting probabilities for each word in the vocabulary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "19eb83ce-e473-43d5-a470-9ab94c457bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the LSTM model\n",
    "#Sequential model allows stacking layers in a linear fashion\n",
    "model = Sequential([\n",
    "  # Embedding layer to convert word indices to dense vectors o\n",
    "  # Input dimension: total number of words, Output dimension:\n",
    "  # Input length: length of input sequences (excluding the las\n",
    "  # 10 means, 10 diamention vector, feature save in the 10 diamention vector, if you have 1000 sentences the resultant vector size 1000*10\n",
    "  #max_sequence_len, if I have sentance of 5 words than I will give 4 words as input, 5th will be pridict by model\n",
    "  Embedding (total_words, 10, input_length=max_sequence_len-1),\n",
    "  #LSTM 100 seq of cells\n",
    "  LSTM(100),\n",
    "  # Dense output layer with a softmax activation function\n",
    "  # Output dimension: total number of words (for multi-class\n",
    "  # softmax is the last layer\n",
    "  Dense(total_words, activation='softmax')\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0b37e120-404e-4080-8aa8-07c9bd4a4260",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile( loss='categorical_crossentropy',optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9591ea72-59bc-43e0-b333-5b09f45251e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 312ms/step - accuracy: 0.0133 - loss: 6.0606\n",
      "Epoch 2/50\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 331ms/step - accuracy: 0.0349 - loss: 5.7012\n",
      "Epoch 3/50\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 338ms/step - accuracy: 0.0420 - loss: 5.6252\n",
      "Epoch 4/50\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 360ms/step - accuracy: 0.0324 - loss: 5.5556\n",
      "Epoch 5/50\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 358ms/step - accuracy: 0.0288 - loss: 5.5348\n",
      "Epoch 6/50\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 352ms/step - accuracy: 0.0397 - loss: 5.5580\n",
      "Epoch 7/50\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 376ms/step - accuracy: 0.0334 - loss: 5.5475\n",
      "Epoch 8/50\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 360ms/step - accuracy: 0.0273 - loss: 5.5597\n",
      "Epoch 9/50\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 373ms/step - accuracy: 0.0355 - loss: 5.4944\n",
      "Epoch 10/50\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 369ms/step - accuracy: 0.0345 - loss: 5.5377\n",
      "Epoch 11/50\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 336ms/step - accuracy: 0.0361 - loss: 5.4498\n",
      "Epoch 12/50\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 336ms/step - accuracy: 0.0539 - loss: 5.3899\n",
      "Epoch 13/50\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 342ms/step - accuracy: 0.0449 - loss: 5.3878\n",
      "Epoch 14/50\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 336ms/step - accuracy: 0.0436 - loss: 5.3212\n",
      "Epoch 15/50\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 317ms/step - accuracy: 0.0484 - loss: 5.2408\n",
      "Epoch 16/50\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 337ms/step - accuracy: 0.0551 - loss: 5.1064\n",
      "Epoch 17/50\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 337ms/step - accuracy: 0.0721 - loss: 5.0126\n",
      "Epoch 18/50\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 329ms/step - accuracy: 0.0642 - loss: 4.8714\n",
      "Epoch 19/50\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 318ms/step - accuracy: 0.0866 - loss: 4.8174\n",
      "Epoch 20/50\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 350ms/step - accuracy: 0.0835 - loss: 4.7531\n",
      "Epoch 21/50\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 344ms/step - accuracy: 0.1003 - loss: 4.5516\n",
      "Epoch 22/50\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 343ms/step - accuracy: 0.1212 - loss: 4.4224\n",
      "Epoch 23/50\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 343ms/step - accuracy: 0.1190 - loss: 4.3062\n",
      "Epoch 24/50\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 341ms/step - accuracy: 0.1283 - loss: 4.1909\n",
      "Epoch 25/50\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 336ms/step - accuracy: 0.1512 - loss: 4.0473\n",
      "Epoch 26/50\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 342ms/step - accuracy: 0.1765 - loss: 3.8353\n",
      "Epoch 27/50\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 352ms/step - accuracy: 0.1969 - loss: 3.7573\n",
      "Epoch 28/50\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 343ms/step - accuracy: 0.2136 - loss: 3.5722\n",
      "Epoch 29/50\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 320ms/step - accuracy: 0.2297 - loss: 3.4798\n",
      "Epoch 30/50\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 340ms/step - accuracy: 0.2217 - loss: 3.3972\n",
      "Epoch 31/50\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 341ms/step - accuracy: 0.2793 - loss: 3.2285\n",
      "Epoch 32/50\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 338ms/step - accuracy: 0.3131 - loss: 3.1536\n",
      "Epoch 33/50\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 327ms/step - accuracy: 0.3106 - loss: 3.0153\n",
      "Epoch 34/50\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 333ms/step - accuracy: 0.3663 - loss: 2.8569\n",
      "Epoch 35/50\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 376ms/step - accuracy: 0.3981 - loss: 2.7696\n",
      "Epoch 36/50\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 307ms/step - accuracy: 0.4375 - loss: 2.6599\n",
      "Epoch 37/50\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 339ms/step - accuracy: 0.4272 - loss: 2.6435\n",
      "Epoch 38/50\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 324ms/step - accuracy: 0.4989 - loss: 2.5288\n",
      "Epoch 39/50\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 329ms/step - accuracy: 0.5114 - loss: 2.3997\n",
      "Epoch 40/50\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 338ms/step - accuracy: 0.5510 - loss: 2.3222\n",
      "Epoch 41/50\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 345ms/step - accuracy: 0.5881 - loss: 2.2283\n",
      "Epoch 42/50\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 332ms/step - accuracy: 0.6137 - loss: 2.1335\n",
      "Epoch 43/50\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 320ms/step - accuracy: 0.6237 - loss: 2.0731\n",
      "Epoch 44/50\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 350ms/step - accuracy: 0.6680 - loss: 2.0052\n",
      "Epoch 45/50\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 346ms/step - accuracy: 0.6638 - loss: 1.9188\n",
      "Epoch 46/50\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 344ms/step - accuracy: 0.6848 - loss: 1.8941\n",
      "Epoch 47/50\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 341ms/step - accuracy: 0.6950 - loss: 1.7636\n",
      "Epoch 48/50\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 337ms/step - accuracy: 0.7171 - loss: 1.7272\n",
      "Epoch 49/50\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 322ms/step - accuracy: 0.7306 - loss: 1.7033\n",
      "Epoch 50/50\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 337ms/step - accuracy: 0.7497 - loss: 1.6112\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2b325643a10>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=50, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d15596-6413-44b4-92e8-3ea2ec9e7831",
   "metadata": {},
   "source": [
    "### **Function to Predict the Next Word(s)**\n",
    "\n",
    "The `predict_next_word` function generates text based on a seed text input. It can predict one or multiple words, depending on the `next_words` parameter.\n",
    "\n",
    "1. **Convert to Sequence**: The seed text is converted into a sequence of integers using the tokenizer and padded to the required length.\n",
    "\n",
    "2. **Predict Next Word**: The model predicts the probabilities for the next word. The word with the highest probability is selected and appended to the seed text.\n",
    "\n",
    "3. **Repeat for Multiple Words**: The process repeats for the number of words specified by `next_words`, generating and appending each predicted word to the seed text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5475a894-90a7-4381-8ca5-0618b45ec003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict the next word(s) given a seed text\n",
    "# next_words mean predict 1 one word, if set next_words=2 it means predict two words\n",
    "def predict_next_word(seed_text, next_words=2):\n",
    "    for _ in range(next_words):\n",
    "      # Convert the seed text to a sequence of integers\n",
    "      token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "      # Pad the sequence to match the input length required\n",
    "      token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "      # Predict the probabilities of the next word in the se\n",
    "      predicted = model.predict(token_list, verbose=0)\n",
    "      # Get the index of the word with the highest probability\n",
    "      predicted_word_index = np.argmax(predicted, axis=-1)[0]\n",
    "      predicted_word = tokenizer.index_word[predicted_word_index]\n",
    "      # Append the predicted word to the seed text\n",
    "      seed_text += \" \" + predicted_word\n",
    "      # Get the word corresponding to the predicted index\n",
    "    return seed_text\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44713fd4-e910-4e99-8a9c-261e80a33a1a",
   "metadata": {},
   "source": [
    "### **Predict and Print Next Word**\n",
    "\n",
    "To predict and print the next word given the seed text \"Artificial Intelligence\" with `next_words=1`, use the following code:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "25df7b4f-4bba-451e-ad2e-610f32cb3286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artifitial Intellegence  learning\n"
     ]
    }
   ],
   "source": [
    "print(predict_next_word(\"Artifitial Intellegence \",1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100d5b01-b320-4bdf-9e34-8fac8472fb5e",
   "metadata": {},
   "source": [
    "### **Conclusion**\n",
    "\n",
    "This project explored the use of Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM) networks for word prediction tasks. Both RNN and LSTM models were implemented to predict the next word(s) given a seed text.\n",
    "\n",
    "### **Key Steps:**\n",
    "- **Data Preparation**: Tokenized sentences, padded sequences, and encoded outputs for model training.\n",
    "- **Model Definition**: Constructed RNN and LSTM models to process sequential data and generate predictions.\n",
    "- **Prediction**: Used models to extend seed text with predicted words.\n",
    "\n",
    "### **Evaluation:**\n",
    "- **Model Performance**: The effectiveness of the RNN and LSTM models was evaluated based on their ability to predict meaningful and contextually appropriate words. \n",
    "- **Accuracy**: Measures such as prediction accuracy and loss were used to assess model performance during training.\n",
    "- **Output Quality**: Predictions were qualitatively assessed by examining the coherence and relevance of the generated text.\n",
    "\n",
    "Overall, the RNN and LSTM models demonstrated their capability to handle sequential data and provide contextually relevant predictions. Future work could involve fine-tuning hyperparameters, exploring more complex architectures, or expanding the dataset to further enhance model performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9453caba-80d0-48e3-83f5-250d9253ff01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (MytestEnv)",
   "language": "python",
   "name": "mytestenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
