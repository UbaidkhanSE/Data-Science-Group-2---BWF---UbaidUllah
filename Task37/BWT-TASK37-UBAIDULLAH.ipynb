{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61b33b77-84b5-4ec8-a0fb-6216803c4447",
   "metadata": {},
   "source": [
    "<h1 style=\"color: #003366; font-family: Arial, sans-serif; font-weight: bold;\">NLP Preprocessing</h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676b7915-7417-4df2-9727-3f84c62e54dc",
   "metadata": {},
   "source": [
    "#### **Objective**\n",
    "\n",
    "The primary goal of the preprocessing steps was to **clean and standardize textual data** from various corpora, preparing them for effective analysis and modeling. Each dataset required unique preprocessing techniques tailored to its specific challenges and characteristics, including:\n",
    "\n",
    "- **Uniform Formatting**: Ensuring consistency in text formatting across different datasets, such as converting text to lowercase and handling contractions.\n",
    "\n",
    "- **Noise Removal**: Eliminating non-essential elements like punctuation, special characters, HTML tags, and URLs to focus on the core content.\n",
    "\n",
    "- **Text Normalization**: Standardizing text by techniques such as stemming, lemmatization, and correcting spelling errors to consolidate different forms of the same word.\n",
    "\n",
    "- **Data Structuring**: Tokenizing text, generating n-grams, and vectorizing text to prepare it for analytical and modeling tasks.\n",
    "\n",
    "- **Contextual Handling**: Addressing dataset-specific issues, such as removing emojis from chat data or expanding acronyms in formal texts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "fb863c34-ca68-4edb-9c17-a700aac394ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import gutenberg, stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import re\n",
    "from textblob import TextBlob \n",
    "from nltk.corpus import inaugural\n",
    "import warnings\n",
    "import emoji\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762b74bf-c3f8-4059-9c6c-9fab6d7b4e2a",
   "metadata": {},
   "source": [
    "#### **Preprocessing Steps for the Brown Corpus Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b083874a-5393-4218-8f10-d8ec7524f9dc",
   "metadata": {},
   "source": [
    "1. **Lowercasing:** \n",
    "   Convert all characters in the text to lowercase, ensuring uniformity and reducing case-related discrepancies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "d88cffd4-3c7a-40d4-a2d7-90d63e805cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowercased Sentence: the fulton county grand jury said friday an investigation of atlanta's recent primary election produced `` no evidence '' that any irregularities took place .\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='nltk')\n",
    "nltk.download('brown', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "sentences = brown.sents()\n",
    "\n",
    "lowercased_sentences = [[word.lower() for word in sentence] for sentence in sentences]\n",
    "print(\"Lowercased Sentence:\", ' '.join(lowercased_sentences[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e04c7e-eb8d-4c5a-bcce-2077ec84fbbd",
   "metadata": {},
   "source": [
    "2. **Tokenization:** \n",
    "   Split the text into individual words or tokens, facilitating easier analysis and processing.\n",
    "3. **Removing Punctuation:** \n",
    "   Eliminate non-essential symbols from the text to focus on the words themselves and clean the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "92c470dd-edb1-4ef1-b455-582f8efaf063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Sentence: ['the', 'fulton', 'county', 'grand', 'jury', 'said', 'friday', 'an', 'investigation', 'of', 'atlanta', 's', 'recent', 'primary', 'election', 'produced', 'no', 'evidence', 'that', 'any', 'irregularities', 'took', 'place']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokenized_sentences = [tokenizer.tokenize(' '.join(sentence)) for sentence in lowercased_sentences]\n",
    "print(\"Tokenized Sentence:\", tokenized_sentences[0])\n",
    "# Removing Punctuation is already included in tokenization using RegexpTokenizerThis step is already included in tokenization using RegexpTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8582b3-4e16-45d1-8d95-90041ac66546",
   "metadata": {},
   "source": [
    "4. **Removing Stop Words:** \n",
    "   Filter out common words like \"and\" or \"the\" that carry less semantic weight, enhancing the relevance of the remaining content.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "a9b22911-30cc-429b-92d9-551e87642fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Sentence (Stop Words Removed): fulton county grand jury said friday investigation atlanta recent primary election produced evidence irregularities took place\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_sentences = [[word for word in sentence if word not in stop_words] for sentence in tokenized_sentences]\n",
    "print(\"Filtered Sentence (Stop Words Removed):\", ' '.join(filtered_sentences[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421b462e-9564-4c02-94f8-a3612d517c8e",
   "metadata": {},
   "source": [
    "5. **Stemming:** \n",
    "   Reduce words to their base or root forms to consolidate different inflections into a single root, improving text analysis efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "7efdb166-99a9-4aa7-967d-fe160cc2276a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed Sentence: fulton counti grand juri said friday investig atlanta recent primari elect produc evid irregular took place\n"
     ]
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stemmed_sentences = [[stemmer.stem(word) for word in sentence] for sentence in filtered_sentences]\n",
    "print(\"Stemmed Sentence:\", ' '.join(stemmed_sentences[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da058cad-3fe7-43b2-92e4-96fb8a284260",
   "metadata": {},
   "source": [
    "#### **Preprocessing Steps for the Gutenberg Corpus Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8046fce-8def-4b51-a3fe-2cc0906d48a4",
   "metadata": {},
   "source": [
    "6. **Lemmatization:** \n",
    "   Convert words to their dictionary form (e.g., \"better\" to \"good\") to standardize word usage and improve text consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "8ae0b3cd-ebc5-438c-993e-4b129694ccaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('gutenberg', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "text = gutenberg.raw('austen-emma.txt')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "b2be2d12-779e-42e0-bdc5-c843e809e662",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_word(word):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    # Determine the part of speech for the word\n",
    "    pos = wordnet.VERB if wordnet.synsets(word, pos=wordnet.VERB) else wordnet.NOUN\n",
    "    return lemmatizer.lemmatize(word, pos=pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbe2db3-2291-41c5-840c-02efea6be0ac",
   "metadata": {},
   "source": [
    "7. **Removing Numbers:** \n",
    "   Eliminate numerical digits from the text to focus on the linguistic content and remove non-textual elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "f32e40f2-9739-44cf-a737-96b6971c691a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_numbers(text):\n",
    "    return re.sub(r'\\d+', '', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842b6d89-9abf-41dd-a513-5ac1acc70074",
   "metadata": {},
   "source": [
    "8. **Removing Whitespace:** \n",
    "   Trim unnecessary spaces from the text to clean up formatting and ensure a consistent text structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "4f9d3c14-5d13-4c14-8a24-db7c0130bd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_whitespace(text):\n",
    "    return ' '.join(text.split())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b34221e-2a8b-4ec6-9d45-eade2c872d3b",
   "metadata": {},
   "source": [
    "9. **Handling Contractions:** \n",
    "   Expand contractions (e.g., \"can't\" to \"cannot\") to ensure that all text is in its full form, enhancing readability and uniformity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "8e05f5f3-e67f-4e65-95a9-7ab6e0a32e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(text):\n",
    "    contractions = {\n",
    "        \"can't\": \"cannot\", \"won't\": \"will not\", \"n't\": \" not\",\n",
    "        \"'m\": \" am\", \"'re\": \" are\", \"'s\": \" is\", \"'d\": \" would\",\n",
    "        \"'ll\": \" will\", \"'t\": \" not\", \"'ve\": \" have\", \"'y\": \" you\",\n",
    "        \"'d\": \" had\"\n",
    "    }\n",
    "    pattern = re.compile(r\"\\b(\" + \"|\".join(contractions.keys()) + r\")\\b\")\n",
    "    return pattern.sub(lambda x: contractions[x.group()], text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efd2282-d644-426b-8e34-441eca3d1768",
   "metadata": {},
   "source": [
    "10. **Text Normalization:** \n",
    "   Normalize different spellings of the same word (e.g., \"color\" and \"colour\") to unify the text and reduce variability in word forms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "17e15980-d129-4324-8b28-7812e92a3b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    return text.lower().replace('colour', 'color')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "86131b43-1352-423e-95b4-5688a851f939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokens after preprocessing: ['emma', 'by', 'jane', 'austen', 'volume', 'i', 'chapter', 'i', 'emma', 'woodhouse', 'handsome', 'clever', 'and', 'rich', 'with', 'a', 'comfortable', 'home', 'and', 'happy', 'disposition', 'seem', 'to', 'unite', 'some', 'of', 'the', 'best', 'blessing', 'of', 'existence', 'and', 'have', 'live', 'nearly', 'twenty', 'one', 'year', 'in', 'the', 'world', 'with', 'very', 'little', 'to', 'distress', 'or', 'vex', 'her', 'she']\n"
     ]
    }
   ],
   "source": [
    "text = expand_contractions(text)\n",
    "text = remove_numbers(text)\n",
    "text = remove_whitespace(text)\n",
    "text = normalize_text(text)\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokens = tokenizer.tokenize(text)\n",
    "tokens = [lemmatize_word(token) for token in tokens if token.isalpha()]  # Lemmatize and remove non-alphabetic tokens\n",
    "\n",
    "print(\"Sample tokens after preprocessing:\", tokens[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37624e3-b0fc-4ebb-99e5-8bda584c7b48",
   "metadata": {},
   "source": [
    "#### **Preprocessing Steps for the Movie Reviews Corpus**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ec10d7-915b-42ee-b489-7c276ecd2cd7",
   "metadata": {},
   "source": [
    "11. **Removing Special Characters:** \n",
    "   Remove special characters (e.g., `@`, `#`, `$`) from the text to focus on the linguistic content and eliminate non-textual elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "6fef61f2-e1e0-482f-b539-9d8ede5acf95",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Check out this great deal @ http://example.com! Contact us via email@example.com. \n",
    "Our latest offer is #awesome. Don't miss this! <b>Special offer</b> just for you.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "1876c307-282a-4c75-b196-2171c9cde21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_characters(text):\n",
    "    return re.sub(r'[^a-zA-Z0-9\\s]', '', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fef7690-25a7-4c84-a312-05ff8caa4960",
   "metadata": {},
   "source": [
    "12. **Spelling Correction:** \n",
    "   Automatically correct spelling mistakes in the text to enhance readability and uniformity by fixing common typos and misspellings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "34a3af1e-9bcc-4ab9-b9d5-4466d64d46e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_spelling(text):\n",
    "    blob = TextBlob(text)\n",
    "    return str(blob.correct())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91589be0-1149-4a26-a3ec-870bb2ac8589",
   "metadata": {},
   "source": [
    "13. **Removing HTML Tags:** \n",
    "   Remove HTML tags from the text to clean up formatting and retain only the visible content, eliminating unnecessary HTML elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "1902d857-6e79-4660-bb40-fe52b172390a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_tags(text):\n",
    "    return re.sub(r'<.*?>', '', text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a2b9d3-a796-44eb-901b-9f3d4c8eb33f",
   "metadata": {},
   "source": [
    "14. **Removing URLs:** \n",
    "   Remove URLs (web links) from the text to avoid including web addresses that are irrelevant to the textual analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "ee3d7914-c9c8-47e5-9c98-a2f654578924",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls(text):\n",
    "    return re.sub(r'http[s]?://\\S+', '', text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2c7a40-75cb-4d4e-87e0-324964c6d3d3",
   "metadata": {},
   "source": [
    "15. **Removing Email Addresses:** \n",
    "   Remove email addresses from the text to exclude personal or contact information that may not be pertinent to the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "f8457f38-bcdd-4881-b989-6fd634a87c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_email_addresses(text):\n",
    "    return re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "b9ff7810-cf2c-4945-9aa8-094c2d43c232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: \n",
      "Check out this great deal @ http://example.com! Contact us via email@example.com. \n",
      "Our latest offer is #awesome. Don't miss this! <b>Special offer</b> just for you.\n",
      "\n",
      "Text after Removing Special Characters: \n",
      "Check out this great deal  httpexamplecom Contact us via emailexamplecom \n",
      "Our latest offer is awesome Dont miss this bSpecial offerb just for you\n",
      "\n",
      "Text after Spelling Correction: \n",
      "Check out this great deal  httpexamplecom Contact us via emailexamplecom \n",
      "Our latest offer is awesome Wont miss this special offer just for you\n",
      "\n",
      "Text after Removing HTML Tags: \n",
      "Check out this great deal  httpexamplecom Contact us via emailexamplecom \n",
      "Our latest offer is awesome Wont miss this special offer just for you\n",
      "\n",
      "Text after Removing URLs: \n",
      "Check out this great deal  httpexamplecom Contact us via emailexamplecom \n",
      "Our latest offer is awesome Wont miss this special offer just for you\n",
      "\n",
      "Text after Removing Email Addresses: \n",
      "Check out this great deal  httpexamplecom Contact us via emailexamplecom \n",
      "Our latest offer is awesome Wont miss this special offer just for you\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_no_special_chars = remove_special_characters(text)\n",
    "text_corrected_spelling = correct_spelling(text_no_special_chars)\n",
    "text_no_html = remove_html_tags(text_corrected_spelling)\n",
    "text_no_urls = remove_urls(text_no_html)\n",
    "text_no_emails = remove_email_addresses(text_no_urls)\n",
    "\n",
    "print(\"Original Text:\", text)\n",
    "print(\"Text after Removing Special Characters:\", text_no_special_chars)\n",
    "print(\"Text after Spelling Correction:\", text_corrected_spelling)\n",
    "print(\"Text after Removing HTML Tags:\", text_no_html)\n",
    "print(\"Text after Removing URLs:\", text_no_urls)\n",
    "print(\"Text after Removing Email Addresses:\", text_no_emails)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffcc482-1a42-4d64-bbec-1fd722d48e73",
   "metadata": {},
   "source": [
    "#### **Preprocessing Steps for the Inaugural Address Corpus**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89165231-3d07-4213-90c4-60d45dc45500",
   "metadata": {},
   "source": [
    "16. **Removing Non-ASCII Characters:** \n",
    "   Remove characters that are not part of the ASCII character set to ensure uniformity and avoid issues with non-standard characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "bc34f4c8-0bfb-4c77-a000-aeda7cdf575d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='nltk')\n",
    "nltk.download('inaugural', quiet=True)\n",
    "nltk.download('punkt', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "c320a5b1-8b90-469f-8cbe-be89350c231f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    return inaugural.raw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "b2f1f018-819f-4cf1-b664-fb0d14c6c257",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_ascii(text):\n",
    "    return ''.join(char for char in text if ord(char) < 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67f935c-9fdd-4913-8f51-c2791f075b33",
   "metadata": {},
   "source": [
    "17. **Removing Repeated Characters:** \n",
    "   Reduce repeated characters (e.g., \"soooo\" to \"so\") to standardize text and avoid excessive character repetitions that may affect readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "b9f851dc-161f-4dcc-96de-7f79def6ed64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_repeated_characters(text):\n",
    "    return re.sub(r'(.)\\1+', r'\\1\\1', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe63762-273a-454d-8287-7f3e0a1c1e6c",
   "metadata": {},
   "source": [
    "18. **Expanding Acronyms:** \n",
    "   Expand commonly used acronyms (e.g., \"AI\" to \"Artificial Intelligence\") to enhance clarity and ensure that all abbreviations are fully spelled out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "b358dfe3-c64d-4155-adca-eb4fee5a8b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_acronyms(text):\n",
    "    acronyms = {\n",
    "        'AI': 'Artificial Intelligence',\n",
    "        'USA': 'United States of America',\n",
    "    }\n",
    "    for acronym, expansion in acronyms.items():\n",
    "        text = text.replace(acronym, expansion)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391c6594-5796-456d-8834-988af4424e4c",
   "metadata": {},
   "source": [
    "19. **Removing Affixes:** \n",
    "   Remove prefixes or suffixes from words to focus on the root forms and improve the consistency of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "a596c444-ebca-4c74-9817-cb0aaf71fa72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_affixes(text):\n",
    "    affixes = ['un', 're', 'ing', 'ed', 'ly', 's', 'es']\n",
    "    for affix in affixes:\n",
    "        text = re.sub(r'\\b' + affix + r'\\b', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d2cac7-b9a7-4fbf-9b84-e7e8509caad8",
   "metadata": {},
   "source": [
    "20. **Removing Extra Spaces:** \n",
    "   Remove extra spaces between words to clean up formatting and ensure a consistent text structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "514b85f9-db29-46c0-87cf-8a791d8d13ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_extra_spaces(text):\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "eae60e7a-9bd0-4dfd-b411-3d9af3860228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text Sample:\n",
      "Fellow-Citizens of the Senate and of the House of Representatives:\n",
      "\n",
      "Among the vicissitudes incident to life no event could have filled me with greater anxieties than that of which the notification was transmitted by your order, and received on the 14th day of the present month. On the one hand, I was summoned by my Country, whose voice I can never hear but with veneration and love, from a retreat which I had chosen with the fondest predilection, and, in my flattering hopes, with an immutable decision, as the asylum of my declining years -- a retreat which was rendered every day more necessary as well as more dear to me by the addition of habit to inclination, and of frequent interruptions in my health to the gradual waste committed on it by time. On the other hand, the magnitude and difficulty of the trust to which the voice of my country called me, being sufficient to awaken in the wisest and most experienced of her citizens a distrustful scrutiny into his qualifications, could not bu\n",
      "\n",
      "Preprocessed Text Sample:\n",
      "Fellow-Citizens of the Senate and of the House of Representatives: Among the vicissitudes incident to life no event could have filled me with greater anxieties than that of which the notification was transmitted by your order, and received on the 14th day of the present month. On the one hand, I was summoned by my Country, whose voice I can never hear but with veneration and love, from a retreat which I had chosen with the fondest predilection, and, in my flattering hopes, with an immutable decision, as the asylum of my declining years -- a retreat which was rendered every day more necessary as well as more dear to me by the addition of habit to inclination, and of frequent interruptions in my health to the gradual waste committed on it by time. On the other hand, the magnitude and difficulty of the trust to which the voice of my country called me, being sufficient to awaken in the wisest and most experienced of her citizens a distrustful scrutiny into his qualifications, could not but\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    text = remove_non_ascii(text)\n",
    "    text = remove_repeated_characters(text)\n",
    "    text = expand_acronyms(text)\n",
    "    text = remove_affixes(text)\n",
    "    text = remove_extra_spaces(text)\n",
    "    return text\n",
    "\n",
    "text = load_dataset()\n",
    "preprocessed_text = preprocess_text(text)\n",
    "\n",
    "print(\"Original Text Sample:\")\n",
    "print(text[:1000])  \n",
    "\n",
    "print(\"\\nPreprocessed Text Sample:\")\n",
    "print(preprocessed_text[:1000])  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3054197-c7ce-45a4-aca5-96b1be1f1c41",
   "metadata": {},
   "source": [
    "#### **Text Processing Steps for the NPS Chat Corpus**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0ae1d3-d956-4a23-88b6-0bb91fc7effa",
   "metadata": {},
   "source": [
    "21. **Handling Emojis:** \n",
    "   Remove emojis from the text to ensure uniformity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "1be5ec0f-b274-4df6-aaaf-e4c497d1cc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "f67c98ea-e0ad-4bba-91bd-38ecc559e01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_emojis(text):\n",
    "    return emoji.replace_emoji(text, replace='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b59205-6807-4315-a91d-3ba184e899e9",
   "metadata": {},
   "source": [
    "22. **Sentence Splitting:** \n",
    "   Split the text into individual sentences using regular expressions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "2fa2c346-6a9f-474d-b001-28519614148c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sentences(text):\n",
    "    return re.split(r'(?<=[.!?]) +', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ef115f-f2a9-4b13-97d2-bc8d654bb93f",
   "metadata": {},
   "source": [
    "23. **Creating N-grams:** \n",
    "   Generate bigrams and trigrams by tokenizing the text and combining tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "15f2ad3b-2e0b-4564-ac61-a649b8c814b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ngrams(text, n):\n",
    "    tokens = text.split()\n",
    "    ngrams = zip(*[tokens[i:] for i in range(n)])\n",
    "    return list(ngrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e94cb22-532e-47e1-b6ee-3db24638dffd",
   "metadata": {},
   "source": [
    "24. **Named Entity Recognition (NER) and Part-of-Speech (POS) Tagging:** \n",
    "   Identify named entities and assign part-of-speech tags to words using spaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "51f6d8d4-2474-416d-ab3d-5eda26d99977",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_ner_and_pos(text):\n",
    "    doc = nlp(text)\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    pos_tags = [(token.text, token.pos_) for token in doc]\n",
    "    return entities, pos_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "d70ffd44-7a7e-4f96-b0af-c4e6fffa0896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text after removing emojis:\n",
      "Hello ! How are you?  Let's meet at 5:00 PM.\n",
      "\n",
      "Sample Sentences:\n",
      "['Hello !', 'How are you?', \"Let's meet at 5:00 PM.\"]\n",
      "\n",
      "Bigrams:\n",
      "[('Hello', '!'), ('!', 'How'), ('How', 'are'), ('are', 'you?'), ('you?', \"Let's\"), (\"Let's\", 'meet'), ('meet', 'at'), ('at', '5:00'), ('5:00', 'PM.')]\n",
      "\n",
      "Trigrams:\n",
      "[('Hello', '!', 'How'), ('!', 'How', 'are'), ('How', 'are', 'you?'), ('are', 'you?', \"Let's\"), ('you?', \"Let's\", 'meet'), (\"Let's\", 'meet', 'at'), ('meet', 'at', '5:00'), ('at', '5:00', 'PM.')]\n",
      "\n",
      "Named Entities:\n",
      "[('5:00 PM', 'TIME')]\n",
      "\n",
      "Part-of-Speech Tags:\n",
      "[('Hello', 'INTJ'), ('!', 'PUNCT'), ('How', 'SCONJ'), ('are', 'AUX'), ('you', 'PRON'), ('?', 'PUNCT'), (' ', 'SPACE'), ('Let', 'VERB'), (\"'s\", 'PRON'), ('meet', 'VERB'), ('at', 'ADP'), ('5:00', 'NUM'), ('PM', 'NOUN'), ('.', 'PUNCT')]\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello ðŸ˜ƒ! How are you? ðŸ¡ Let's meet at 5:00 PM.\"\n",
    "\n",
    "text_no_emojis = handle_emojis(text)\n",
    "print(\"Text after removing emojis:\")\n",
    "print(text_no_emojis)\n",
    "\n",
    "sentences = split_sentences(text_no_emojis)\n",
    "print(\"\\nSample Sentences:\")\n",
    "print(sentences)\n",
    "\n",
    "try:\n",
    "    bigrams = create_ngrams(text_no_emojis, 2)\n",
    "    trigrams = create_ngrams(text_no_emojis, 3)\n",
    "    print(\"\\nBigrams:\")\n",
    "    print(bigrams)\n",
    "    print(\"\\nTrigrams:\")\n",
    "    print(trigrams)\n",
    "except Exception as e:\n",
    "    print(\"\\nError creating n-grams:\", e)\n",
    "\n",
    "entities, pos_tags = perform_ner_and_pos(text_no_emojis)\n",
    "print(\"\\nNamed Entities:\")\n",
    "print(entities)\n",
    "print(\"\\nPart-of-Speech Tags:\")\n",
    "print(pos_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bf4dd1-b160-4318-8c81-d7db2f528cc8",
   "metadata": {},
   "source": [
    "#### **Preprocessing Steps for the Movie Reviews Corpus**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbca0e7-47c6-4f9c-9c12-7f6282feb0e1",
   "metadata": {},
   "source": [
    "25. **Text Vectorization (e.g., TF-IDF):** \n",
    "   Convert text into numerical features using methods like TF-IDF.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "19b8d736-b091-4406-84c3-3a0766fc7c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "This is a sample text. Here is an example of CamelCase format: CamelCase.\n",
    "HTML entities like &amp; should be converted. \n",
    "Let's also remove line breaks and convert multi-line text into a single line.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "977e90ec-48bc-44b1-858c-a447c9c43296",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(text):\n",
    "    \"\"\"Convert text into numerical features using TF-IDF.\"\"\"\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform([text])\n",
    "    return X, vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdeaaa33-fac5-4581-9869-d8d82cb538a9",
   "metadata": {},
   "source": [
    "26. **Removing HTML Entities:** \n",
    "   Convert HTML entities (e.g., `&amp;`) to their corresponding characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "e309716b-dde8-4cc3-a85c-d70e016fed63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_entities(text):\n",
    "    \"\"\"Convert HTML entities (e.g., `&amp;`) to their corresponding characters.\"\"\"\n",
    "    return re.sub(r'&\\w+;', '', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed77e94-0279-43df-ba54-6d2f3313366f",
   "metadata": {},
   "source": [
    "27. **Lowercasing the First Word of Sentences:** \n",
    "   Lowercase only the first word of each sentence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "74eeb02d-0d39-4bc1-a0cf-aad1d464ffcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowercase_first_word(text):\n",
    "    \"\"\"Lowercase only the first word of each sentence.\"\"\"\n",
    "    sentences = re.split(r'(?<=[.!?]) +', text)\n",
    "    sentences = [s[0].lower() + s[1:] if s else '' for s in sentences]\n",
    "    return ' '.join(sentences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9eb7ac-4331-4e97-b05c-31fb96d62d88",
   "metadata": {},
   "source": [
    "28. **Splitting Words from CamelCase:** \n",
    "   Split words that are in CamelCase format (e.g., \"CamelCase\" to \"Camel Case\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "d94108b3-3131-4146-917d-2f51a0311dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_camel_case(text):\n",
    "    \"\"\"Split words that are in CamelCase format (e.g., \"CamelCase\" to \"Camel Case\").\"\"\"\n",
    "    return re.sub(r'([a-z])([A-Z])', r'\\1 \\2', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cb053f-bc5c-4ad0-8db7-b4ad4ace26c0",
   "metadata": {},
   "source": [
    "29. **Removing Line Breaks:** \n",
    "   Remove line breaks and convert multi-line text into a single line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "5bdbb4bc-dad7-4cf6-87d9-cfacba07ecbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_line_breaks(text):\n",
    "    \"\"\"Remove line breaks and convert multi-line text into a single line.\"\"\"\n",
    "    return text.replace('\\n', ' ').replace('\\r', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "368e4aca-75d9-48a2-9359-813f18916a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After removing HTML entities:\n",
      "\n",
      "This is a sample text. Here is an example of CamelCase format: CamelCase.\n",
      "HTML entities like  should be converted. \n",
      "Let's also remove line breaks and convert multi-line text into a single line.\n",
      "\n",
      "\n",
      "After lowercasing the first word of each sentence:\n",
      "\n",
      "This is a sample text. here is an example of CamelCase format: CamelCase.\n",
      "HTML entities like  should be converted. \n",
      "Let's also remove line breaks and convert multi-line text into a single line.\n",
      "\n",
      "\n",
      "After splitting CamelCase words:\n",
      "\n",
      "This is a sample text. here is an example of Camel Case format: Camel Case.\n",
      "HTML entities like  should be converted. \n",
      "Let's also remove line breaks and convert multi-line text into a single line.\n",
      "\n",
      "\n",
      "After removing line breaks:\n",
      " This is a sample text. here is an example of Camel Case format: Camel Case. HTML entities like  should be converted.  Let's also remove line breaks and convert multi-line text into a single line. \n",
      "\n",
      "TF-IDF Matrix:\n",
      "[[0.14586499 0.14586499 0.14586499 0.14586499 0.14586499 0.29172998\n",
      "  0.29172998 0.14586499 0.14586499 0.14586499 0.14586499 0.14586499\n",
      "  0.14586499 0.14586499 0.14586499 0.29172998 0.14586499 0.14586499\n",
      "  0.43759497 0.14586499 0.14586499 0.14586499 0.14586499 0.14586499\n",
      "  0.14586499 0.29172998 0.14586499]]\n",
      "\n",
      "Feature Names:\n",
      "['also' 'an' 'and' 'be' 'breaks' 'camel' 'case' 'convert' 'converted'\n",
      " 'entities' 'example' 'format' 'here' 'html' 'into' 'is' 'let' 'like'\n",
      " 'line' 'multi' 'of' 'remove' 'sample' 'should' 'single' 'text' 'this']\n"
     ]
    }
   ],
   "source": [
    "text_no_html = remove_html_entities(text)\n",
    "print(\"After removing HTML entities:\")\n",
    "print(text_no_html)\n",
    "\n",
    "text_lowercased = lowercase_first_word(text_no_html)\n",
    "print(\"\\nAfter lowercasing the first word of each sentence:\")\n",
    "print(text_lowercased)\n",
    "\n",
    "text_split_camel_case = split_camel_case(text_lowercased)\n",
    "print(\"\\nAfter splitting CamelCase words:\")\n",
    "print(text_split_camel_case)\n",
    "\n",
    "text_single_line = remove_line_breaks(text_split_camel_case)\n",
    "print(\"\\nAfter removing line breaks:\")\n",
    "print(text_single_line)\n",
    "\n",
    "tfidf_matrix, feature_names = vectorize_text(text_single_line)\n",
    "print(\"\\nTF-IDF Matrix:\")\n",
    "print(tfidf_matrix.toarray())\n",
    "\n",
    "print(\"\\nFeature Names:\")\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cb57cd-6b52-4318-a364-d6185ec2a1e5",
   "metadata": {},
   "source": [
    "#### **Preprocessing Steps for the State of the Union Corpus**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2259ddd5-e41b-4400-947a-dc0e30012ada",
   "metadata": {},
   "source": [
    "30. **Token Normalization** Normalize different forms of the same token (e.g., \"u.s.a.\" and \"usa\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "df4d94b5-5d81-44e4-8fe2-9a8be954e5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "The U.S.A. is also known as USA. Dr. Smith and Mr. Jones are attending the meeting. The terrorist attack was carried out by a terrorist organization. \n",
    "The quick brown fox jumps over the lazy dog. The service was not good and the experience was not happy.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "3b8c356c-1f81-42c5-a2c5-fc0a81e56f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_tokens(text):\n",
    "    \"\"\"Normalize different forms of the same token.\"\"\"\n",
    "    text = re.sub(r'\\b(u\\.s\\.a\\.|usa)\\b', 'usa', text, flags=re.IGNORECASE)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c510748-fd38-4db9-b8bd-2e55a5ff2bc0",
   "metadata": {},
   "source": [
    "31. **Handling Abbreviations** Expand or normalize abbreviations (e.g., \"Dr.\" to \"Doctor\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "ccbf2d22-6294-457a-8f03-5dd3b1c6e5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_abbreviations(text):\n",
    "    \"\"\"Expand or normalize abbreviations.\"\"\"\n",
    "    abbreviations = {\n",
    "        'Dr.': 'Doctor',\n",
    "        'Mr.': 'Mister',\n",
    "        'Mrs.': 'Missus',\n",
    "        'Gov.': 'Governor'\n",
    "    }\n",
    "    for abbr, full in abbreviations.items():\n",
    "        text = text.replace(abbr, full)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fd57df-6efa-4954-8d34-45357035b87e",
   "metadata": {},
   "source": [
    "32. **Context-Based Replacement** Replace certain words or phrases based on their context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "4dd3ccca-71cd-477b-a77e-5e720a741502",
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_based_replacement(text):\n",
    "    \"\"\"Replace certain words or phrases based on context.\"\"\"\n",
    "    text = re.sub(r'\\bterrorist\\b(?=.*attack)', 'extremist', text, flags=re.IGNORECASE)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b54af64-e401-40b4-bc42-dec59282c850",
   "metadata": {},
   "source": [
    "33. **Removing Stop Words with Custom List** Remove stop words using a custom list of stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "364f1160-4a2b-449f-8d9e-9d579d28a0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_custom_stop_words(text, custom_stop_words):\n",
    "    \"\"\"Remove stop words using a custom list.\"\"\"\n",
    "    words = text.lower().split()\n",
    "    words = [word for word in words if word not in custom_stop_words]\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779eacdb-573a-456e-99db-a0368889b071",
   "metadata": {},
   "source": [
    "34. **Handling Negations**  Process sentences with negations (e.g., \"not good\" to \"bad\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "33f961c2-8006-46ff-b691-78baa03454a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_negations(text):\n",
    "    \"\"\"Process sentences with negations.\"\"\"\n",
    "    negation_patterns = {\n",
    "        r'\\bnot good\\b': 'bad',\n",
    "        r'\\bnot happy\\b': 'unhappy'\n",
    "    }\n",
    "    for pattern, replacement in negation_patterns.items():\n",
    "        text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463111ab-52ff-4f95-aa5f-4af951c95f9b",
   "metadata": {},
   "source": [
    "35. **Text Augmentation** Generate new text data by making slight alterations to existing text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "a866ed94-7772-4ef8-b150-a0cf59bd9f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_augmentation(text):\n",
    "    \"\"\"Generate new text data by making slight alterations.\"\"\"\n",
    "    augmentations = [\n",
    "        lambda x: x + \" Additionally, the text can be altered.\",\n",
    "        lambda x: \"Note: \" + x,\n",
    "        lambda x: x.upper()\n",
    "    ]\n",
    "    return random.choice(augmentations)(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a5b33e-6693-4895-bd03-ec793cf19210",
   "metadata": {},
   "source": [
    "36. **Removing Duplicates** Remove duplicate words or phrases in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "4d632a88-e812-4988-9d8b-4cc23f02c6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(text):\n",
    "    \"\"\"Remove duplicate words or phrases in the text.\"\"\"\n",
    "    words = text.lower().split()\n",
    "    seen = set()\n",
    "    return ' '.join([word for word in words if not (word in seen or seen.add(word))])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bad6a3f-e515-4687-8736-b3b560cf601f",
   "metadata": {},
   "source": [
    "37. **Text Clustering** Cluster similar text segments based on their content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "7f4eb644-1dff-4703-88fb-c61d118165ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_text_clustering(text, num_clusters=2):\n",
    "    \"\"\"Cluster similar text segments (simple example).\"\"\"\n",
    "    # Simple clustering by splitting into parts\n",
    "    sentences = text.split('. ')\n",
    "    clusters = {i: [] for i in range(num_clusters)}\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        cluster_id = i % num_clusters\n",
    "        clusters[cluster_id].append(sentence.strip())\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "6c5f7c08-7f84-4445-bbdc-3165c48abdb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed Text:\n",
      "note: u.s.a. is also known as usa. doctor smith and mister jones are attending meeting. extremist attack was carried out by a terrorist organization. quick brown fox jumps lazy dog. service bad experience unhappy.\n",
      "\n",
      "Text Clusters:\n",
      "Cluster 0:\n",
      "  - note: u.s.a\n",
      "  - doctor smith and mister jones are attending meeting\n",
      "  - quick brown fox jumps lazy dog\n",
      "Cluster 1:\n",
      "  - is also known as usa\n",
      "  - extremist attack was carried out by a terrorist organization\n",
      "  - service bad experience unhappy.\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    text = normalize_tokens(text)\n",
    "    text = expand_abbreviations(text)\n",
    "    text = context_based_replacement(text)\n",
    "    custom_stop_words = set(['the', 'over'])  # Custom stop words list\n",
    "    text = remove_custom_stop_words(text, custom_stop_words)\n",
    "    text = handle_negations(text)\n",
    "    text = text_augmentation(text)\n",
    "    text = remove_duplicates(text)\n",
    "    clusters = simple_text_clustering(text)\n",
    "    return text, clusters\n",
    "\n",
    "\n",
    "preprocessed_text, text_clusters = preprocess_text(text)\n",
    "\n",
    "print(\"Preprocessed Text:\")\n",
    "print(preprocessed_text)\n",
    "\n",
    "print(\"\\nText Clusters:\")\n",
    "for cluster_id, sentences in text_clusters.items():\n",
    "    print(f\"Cluster {cluster_id}:\")\n",
    "    for sentence in sentences:\n",
    "        print(f\"  - {sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bcfed9-599d-43bc-a26c-b8225e45f6a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
